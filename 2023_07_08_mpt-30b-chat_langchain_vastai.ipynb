{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MIT License\n",
    "\n",
    "Copyright (c) 2023 Pavel Shibanov [https://blog.experienced.dev/](https://blog.experienced.dev/?utm_source=notebooks)\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "of this software and associated documentation files (the \"Software\"), to deal\n",
    "in the Software without restriction, including without limitation the rights\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "copies of the Software, and to permit persons to whom the Software is\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [LangChain](https://github.com/hwchase17/langchain) + [MPT-30B-Chat](https://huggingface.co/mosaicml/mpt-30b-chat) with [Text Generation Inference](https://github.com/huggingface/text-generation-inference) on [vast.ai](https://cloud.vast.ai/?ref=71973) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Installing the dependencies\n",
    "%pip install -q python-dotenv==1.0.0 langchain==0.0.226 text-generation==0.6.0 httpx==0.24.1 gradio==3.36.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Load .env file\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Download the [vast.ai](https://cloud.vast.ai/?ref=71973) CLI\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    \"https://raw.githubusercontent.com/vast-ai/vast-python/7d2f10f/vast.py\", \"vast\"\n",
    ")\n",
    "os.chmod(\"vast\", 0o755)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Helper to extract dictionaries from\n",
    "# @markdown [vast.ai CLI](https://cloud.vast.ai/?ref=71973)\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "\n",
    "def vast_ai_cmd(args=None):\n",
    "    if not args:\n",
    "        args = [\"--help\"]\n",
    "    cmd = [\"./vast\"]\n",
    "    cmd += args\n",
    "    result = subprocess.run(cmd, capture_output=True)\n",
    "    if result.stderr:\n",
    "        print(result.stderr)\n",
    "    output = result.stdout.decode()\n",
    "\n",
    "    if \"--raw\" in cmd:\n",
    "        try:\n",
    "            data = json.loads(output)\n",
    "            return data\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"An error occurred: {str(e)}\")\n",
    "    else:\n",
    "        print(output)\n",
    "\n",
    "\n",
    "vast_ai_cmd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown vast.ai API key. You can obtain one at [vast.ai](https://cloud.vast.ai/?ref=71973)\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "VAST_AI_API_KEY = os.getenv(\"VAST_AI_API_KEY\", \"your_vast_ai_api_key\")\n",
    "if VAST_AI_API_KEY == \"your_vast_ai_api_key\":\n",
    "    display(\n",
    "        Markdown(\n",
    "            \"It appears that you don't have a vast.ai API key. You can obtain one at [vast.ai](https://cloud.vast.ai/?ref=71973)\"\n",
    "        )\n",
    "    )\n",
    "    raise AssertionError(\"Missing vast.ai API key\")\n",
    "\n",
    "vast_ai_cmd([\"set\", \"api-key\", VAST_AI_API_KEY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Searching for suitable hardware to run mosaicml/mpt-30b-chat with quantization.\n",
    "import pandas as pd\n",
    "\n",
    "raw_offers = vast_ai_cmd(\n",
    "    [\n",
    "        \"search\",\n",
    "        \"offers\",\n",
    "        \"--raw\",\n",
    "        \"reliability > 0.9 cuda_vers >= 11.8 num_gpus = 1 gpu_ram >= 47 inet_down >= 500 disk_space >= 130\",\n",
    "        \"-o\",\n",
    "        \"dph\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "offers_count = len(raw_offers)\n",
    "if offers_count:\n",
    "    print(f\"{offers_count=}\")\n",
    "    offers = pd.DataFrame.from_dict(raw_offers)\n",
    "else:\n",
    "    raise AssertionError(\"No offers, change search query\")\n",
    "offers[\n",
    "    [\n",
    "        \"id\",\n",
    "        \"dph_total\",\n",
    "        \"inet_down\",\n",
    "        \"cuda_max_good\",\n",
    "        \"num_gpus\",\n",
    "        \"gpu_name\",\n",
    "        \"gpu_ram\",\n",
    "        \"cpu_ram\",\n",
    "        \"disk_space\",\n",
    "    ]\n",
    "].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Selecting a hardware.\n",
    "selected_id = 0\n",
    "selected = offers.iloc[selected_id]\n",
    "selected.id, selected.dph_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Starting [Text Generation Inference](https://github.com/huggingface/text-generation-inference) with [MPT-30B-Chat](https://huggingface.co/mosaicml/mpt-30b-chat) model.\n",
    "res = vast_ai_cmd(\n",
    "    [\n",
    "        \"create\",\n",
    "        \"instance\",\n",
    "        \"--raw\",\n",
    "        str(selected.id),\n",
    "        \"--env\",\n",
    "        \"-p 80:80\",\n",
    "        \"--disk\",\n",
    "        \"130\",\n",
    "        \"--image\",\n",
    "        \"ghcr.io/huggingface/text-generation-inference:0.9.1\",\n",
    "        \"--args\",\n",
    "        \"--model-id\",\n",
    "        \"mosaicml/mpt-30b-chat\",\n",
    "        \"--quantize\",\n",
    "        \"bitsandbytes\",\n",
    "        \"--hostname\",\n",
    "        \"::\",\n",
    "    ]\n",
    ")\n",
    "instance_id = res[\"new_contract\"]\n",
    "instance_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Waiting for instance to become running.\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "is_running = False\n",
    "while not is_running:\n",
    "    instances = vast_ai_cmd([\"show\", \"instances\", \"--raw\"])\n",
    "    instance = next((i for i in instances if i[\"id\"] == instance_id), None)\n",
    "    is_running = instance[\"actual_status\"] == \"running\"\n",
    "    clear_output(wait=True)\n",
    "    print(instance)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Fetching the inference_server_url from instance configuration.\n",
    "hostname = instance[\"public_ipaddr\"].strip()\n",
    "port = instance[\"ports\"][\"80/tcp\"][-1][\"HostPort\"]\n",
    "inference_server_url = f\"http://{hostname}:{port}\"\n",
    "inference_server_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Waiting for the TGI health check and displaying logs every 5 seconds.\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "import httpx\n",
    "\n",
    "url = f\"{inference_server_url}/health\"\n",
    "is_healthy = False\n",
    "while not is_healthy:\n",
    "    try:\n",
    "        res = httpx.get(url)\n",
    "        is_healthy = res.status_code == 200\n",
    "    except:\n",
    "        clear_output(wait=True)\n",
    "        vast_ai_cmd([\"logs\", str(instance_id)])\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Creating llm with LangChain's HuggingFaceTextGenInference\n",
    "from langchain.llms import HuggingFaceTextGenInference\n",
    "\n",
    "llm = HuggingFaceTextGenInference(\n",
    "    inference_server_url=inference_server_url,\n",
    "    max_new_tokens=1024,\n",
    "    top_k=10,\n",
    "    top_p=0.95,\n",
    "    typical_p=0.95,\n",
    "    stop_sequences=[\n",
    "        \"Human:\",\n",
    "        \"AI:\",\n",
    "    ],  # If 'stop_sequences' is not present, the model will generate both sides of the conversation.\n",
    "    temperature=0.1,\n",
    "    repetition_penalty=1.03,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Creating chat memory.\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"history\",\n",
    "    k=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Creating ConversationChain.\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "chat = ConversationChain(llm=llm, memory=memory, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Chat prompt.\n",
    "chat.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Chat prompt template.\n",
    "display(Markdown(chat.prompt.template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown 1st message.\n",
    "display(Markdown(chat.predict(input=\"hi how are you?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown 2nd message.\n",
    "display(Markdown(chat.predict(input=\"My name is Pavel.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Check memory state.\n",
    "chat.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Check if memory is functioning.\n",
    "display(Markdown(chat.predict(input=\"What is my name?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Next question.\n",
    "display(\n",
    "    Markdown(\n",
    "        chat.predict(input=\"What is the difference between a method and a function?\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Follow up question.\n",
    "display(Markdown(chat.predict(input=\"What is more common in OOP?\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Starting Gradio app with the cachatbot.\n",
    "import random\n",
    "import time\n",
    "import gradio as gr\n",
    "\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    memory_key=\"history\",\n",
    "    k=6,\n",
    ")\n",
    "chat = ConversationChain(llm=llm, memory=memory)\n",
    "\n",
    "chatbot = gr.Chatbot()\n",
    "human_message = gr.Textbox()\n",
    "\n",
    "\n",
    "def respond(human_message, chat_history):\n",
    "    ai_message = chat.predict(input=human_message)\n",
    "    chat_history.append((human_message, ai_message))\n",
    "    return \"\", chat_history\n",
    "\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=respond,\n",
    "    examples=[\n",
    "        [\"How to square a circle?\"],\n",
    "        [\"Compare the physical and chemical properties of hydrogen and oxygen\"],\n",
    "        [\"What is the length of human DNA in meters?\"],\n",
    "    ],\n",
    "    inputs=[human_message, chatbot],\n",
    "    outputs=[human_message, chatbot],\n",
    "    allow_flagging=\"never\",\n",
    ")\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "demo.launch(demo.launch(share=IN_COLAB, debug=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Stop the instance.\n",
    "vast_ai_cmd([\"stop\", \"instance\", str(instance_id)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Start the instance.\n",
    "vast_ai_cmd([\"start\", \"instance\", str(instance_id)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Destroy the instance.\n",
    "vast_ai_cmd([\"destroy\", \"instance\", str(instance_id)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @markdown Confirm that you are not charged.\n",
    "vast_ai_cmd([\"show\", \"instances\", \"--raw\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
